"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.FSCache = void 0;
function _stream() {
  const data = _interopRequireDefault(require("stream"));
  _stream = function () {
    return data;
  };
  return data;
}
function _path() {
  const data = _interopRequireDefault(require("path"));
  _path = function () {
    return data;
  };
  return data;
}
function _util() {
  const data = require("util");
  _util = function () {
    return data;
  };
  return data;
}
function _logger() {
  const data = _interopRequireDefault(require("@parcel/logger"));
  _logger = function () {
    return data;
  };
  return data;
}
function _core() {
  const data = require("@parcel/core");
  _core = function () {
    return data;
  };
  return data;
}
var _package = _interopRequireDefault(require("../package.json"));
var _constants = require("./constants");
function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }
// flowlint-next-line untyped-import:off
const pipeline = (0, _util().promisify)(_stream().default.pipeline);
class FSCache {
  /**
   * Maximum number of bytes per large blob file
   */
  #writeLimitChunk;
  constructor(fs, cacheDir, writeLimitChunk = _constants.WRITE_LIMIT_CHUNK) {
    this.fs = fs;
    this.dir = cacheDir;
    this.#writeLimitChunk = writeLimitChunk;
  }
  async ensure() {
    // First, create the main cache directory if necessary.
    await this.fs.mkdirp(this.dir);

    // In parallel, create sub-directories for every possible hex value
    // This speeds up large caches on many file systems since there are fewer files in a single directory.
    let dirPromises = [];
    for (let i = 0; i < 256; i++) {
      dirPromises.push(this.fs.mkdirp(_path().default.join(this.dir, ('00' + i.toString(16)).slice(-2))));
    }
    await Promise.all(dirPromises);
  }
  _getCachePath(cacheId) {
    return _path().default.join(this.dir, cacheId.slice(0, 2), cacheId.slice(2));
  }
  getStream(key) {
    return this.fs.createReadStream(this._getCachePath(`${key}-large`));
  }
  setStream(key, stream) {
    return pipeline(stream, this.fs.createWriteStream(this._getCachePath(`${key}-large`)));
  }
  has(key) {
    return this.fs.exists(this._getCachePath(key));
  }
  getBlob(key) {
    return this.fs.readFile(this._getCachePath(key));
  }
  async setBlob(key, contents) {
    await this.fs.writeFile(this._getCachePath(key), contents);
  }
  async getBuffer(key) {
    try {
      return await this.fs.readFile(this._getCachePath(key));
    } catch (err) {
      if (err.code === 'ENOENT') {
        return null;
      } else {
        throw err;
      }
    }
  }
  #getFilePath(key, index) {
    return _path().default.join(this.dir, `${key}-${index}`);
  }
  async #unlinkChunks(key, index) {
    try {
      await this.fs.unlink(this.#getFilePath(key, index));
      await this.#unlinkChunks(key, index + 1);
    } catch (err) {
      // If there's an error, no more chunks are left to delete
    }
  }
  hasLargeBlob(key) {
    return this.fs.exists(this.#getFilePath(key, 0));
  }
  async getLargeBlob(key) {
    const buffers = [];
    for (let i = 0; await this.fs.exists(this.#getFilePath(key, i)); i += 1) {
      const file = this.fs.readFile(this.#getFilePath(key, i));
      buffers.push(file);
    }
    return Buffer.concat(await Promise.all(buffers));
  }
  async setLargeBlob(key, contents, options) {
    const chunks = Math.ceil(contents.length / this.#writeLimitChunk);
    const writePromises = [];
    if (chunks === 1) {
      // If there's one chunk, don't slice the content
      writePromises.push(this.fs.writeFile(this.#getFilePath(key, 0), contents, {
        signal: options === null || options === void 0 ? void 0 : options.signal
      }));
    } else {
      for (let i = 0; i < chunks; i += 1) {
        writePromises.push(this.fs.writeFile(this.#getFilePath(key, i), typeof contents === 'string' ? contents.slice(i * this.#writeLimitChunk, (i + 1) * this.#writeLimitChunk) : contents.subarray(i * this.#writeLimitChunk, (i + 1) * this.#writeLimitChunk), {
          signal: options === null || options === void 0 ? void 0 : options.signal
        }));
      }
    }

    // If there's already a files following this chunk, it's old and should be removed
    writePromises.push(this.#unlinkChunks(key, chunks));
    await Promise.all(writePromises);
  }
  async deleteLargeBlob(key) {
    const deletePromises = [];
    let i = 0;
    let filePath = this.#getFilePath(key, i);
    while (await this.fs.exists(filePath)) {
      deletePromises.push(this.fs.rimraf(filePath));
      i += 1;
      filePath = this.#getFilePath(key, i);
    }
    await Promise.all(deletePromises);
  }
  async get(key) {
    try {
      let data = await this.fs.readFile(this._getCachePath(key));
      return (0, _core().deserialize)(data);
    } catch (err) {
      if (err.code === 'ENOENT') {
        return null;
      } else {
        throw err;
      }
    }
  }
  async set(key, value) {
    try {
      let blobPath = this._getCachePath(key);
      let data = (0, _core().serialize)(value);
      await this.fs.writeFile(blobPath, data);
    } catch (err) {
      _logger().default.error(err, '@parcel/cache');
    }
  }
  refresh() {
    // NOOP
  }
}
exports.FSCache = FSCache;
(0, _core().registerSerializableClass)(`${_package.default.version}:FSCache`, FSCache);